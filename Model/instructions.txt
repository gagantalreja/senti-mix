**
Model Download Link: https://drive.google.com/file/d/1-A7aWogqR_rHdtAJmYlGFnaFaZ7GdNfp/view?usp=sharing
**
# Use your bennett-email id to access it.

1) In model.py run() function is there.
- call run function by passing array of sentences.
E.g run(['Hi baby',"bye baby"])


2) It will return a class object, It will have these variables as listed below

  Access these variables by doing this

  object.scoreList
  object.score 
  object.textblob
  object.profanity
  object.generatedText
  object.completeText


3) - scoreList is a list with '0th' index is of neutal class score, '1st' index as positive class score and '2nd' index as negative class score.

   - score = argmax of the class.
       {if score==0 : Neutral
        if score==1 : Positive
        if score==2 : Negative}

  - textblob (Score given by textblob library between -1 to 1)

  - profanity (List of bad words found in the sentences)

  - generatedText- Converted Final English Text

  - completeText- contains dictionary of {givenText, hindi translated text and transliterated hindi text}. 


4.) Remove the 'sample' named list [] inside the code in production or make it blank. Once you finalize everything.

5.) Try to make some logic if 2 or more than 2 words exist in the profanity list of the particular sentence than show the model output as negative despite
model prdeicts as positive or neutral or give some weightage to profanity list as well the textblob analyser or do average. Depends on you xD. But make it negative since it will be tested mostly on bad words :P.

6.) See the wireframe file and https://app-meta-searcher.herokuapp.com for layout incase if you need layout for this I'll add. Chhose any dark theme :'(.

7.) Thanks Gagan sir for bearing with my deadliest code :9.

8.) Remove this file from github once it is done.